<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="main.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>
  <title>intelligent-exercises</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div class="card">
          <div class="header" >
        <h1>14. Probabilistic Reasoning</h1>
      </div>
        <div class="contents">
          <p><strong>14.1</strong> We have a bag of three biased coins $a$, $b$, and $c$ with probabilities
          of coming up heads of 20%, 60%, and 80%, respectively. One coin is drawn
          randomly from the bag (with equal likelihood of drawing each of the
          three coins), and then the coin is flipped three times to generate the
          outcomes $X_1$, $X_2$, and $X_3$.</p>
          <ol>
          <li><p>Draw the Bayesian network corresponding to this setup and define the
          necessary CPTs.</p>
          </li>
          <li><p>Calculate which coin was most likely to have been drawn from the bag
          if the observed flips come out heads twice and tails once.</p>
          </li>
          </ol>
          <p><strong>14.2</strong> We have a bag of three biased coins $a$, $b$, and $c$ with probabilities
          of coming up heads of 30%, 60%, and 75%, respectively. One coin is drawn
          randomly from the bag (with equal likelihood of drawing each of the
          three coins), and then the coin is flipped three times to generate the
          outcomes $X_1$, $X_2$, and $X_3$.</p>
          <ol>
          <li><p>Draw the Bayesian network corresponding to this setup and define the
          necessary CPTs.</p>
          </li>
          <li><p>Calculate which coin was most likely to have been drawn from the bag
          if the observed flips come out heads twice and tails once.</p>
          </li>
          </ol>
          <p><strong>14.3</strong> [cpt-equivalence-exercise]
          Equation (<a href="#/">parameter-joint-repn-equation</a>) on
          page <a href="#/">parameter-joint-repn-equation</a> defines the joint distribution represented by a
          Bayesian network in terms of the parameters
          $\theta(X_i{{\,|\,}}{Parents}(X_i))$. This exercise asks you to derive
          the equivalence between the parameters and the conditional probabilities
          ${\textbf{ P}}(X_i{{\,|\,}}{Parents}(X_i))$ from this definition.</p>
          <ol>
          <li><p>Consider a simple network $X\rightarrow Y\rightarrow Z$ with three
          Boolean variables. Use
          Equations (<a href="#/">conditional-probability-equation</a>) and (<a href="#/">marginalization-equation</a>)
          (pages <a href="#/">conditional-probability-equation</a> and <a href="#/">marginalization-equation</a>)
          to express the conditional probability $P(z{{\,|\,}}y)$ as the ratio of two sums, each over entries in the
          joint distribution ${\textbf{P}}(X,Y,Z)$.</p>
          </li>
          <li><p>Now use Equation (<a href="#/">parameter-joint-repn-equation</a>) to
          write this expression in terms of the network parameters
          $\theta(X)$, $\theta(Y{{\,|\,}}X)$, and $\theta(Z{{\,|\,}}Y)$.</p>
          </li>
          <li><p>Next, expand out the summations in your expression from part (b),
          writing out explicitly the terms for the true and false values of
          each summed variable. Assuming that all network parameters satisfy
          the constraint
          $\sum_{x_i} \theta(x_i{{\,|\,}}{parents}(X_i)){{\,{=}\,}}1$, show
          that the resulting expression reduces to $\theta(z{{\,|\,}}y)$.</p>
          </li>
          <li><p>Generalize this derivation to show that
          $\theta(X_i{{\,|\,}}{Parents}(X_i)) = {\textbf{P}}(X_i{{\,|\,}}{Parents}(X_i))$
          for any Bayesian network.</p>
          </li>
          </ol>
          <p><strong>14.4</strong> The <strong>arc reversal</strong> operation of in a Bayesian network allows us to change the direction
          of an arc $X\rightarrow Y$ while preserving the joint probability
          distribution that the network represents @Shachter:1986. Arc reversal
          may require introducing new arcs: all the parents of $X$ also become
          parents of $Y$, and all parents of $Y$ also become parents of $X$.</p>
          <ol>
          <li><p>Assume that $X$ and $Y$ start with $m$ and $n$ parents,
          respectively, and that all variables have $k$ values. By calculating
          the change in size for the CPTs of $X$ and $Y$, show that the total
          number of parameters in the network cannot decrease during
          arc reversal. (<em>Hint</em>: the parents of $X$ and $Y$ need
          not be disjoint.)</p>
          </li>
          <li><p>Under what circumstances can the total number remain constant?</p>
          </li>
          <li><p>Let the parents of $X$ be $\textbf{U} \cup \textbf{V}$ and the parents of $Y$ be
          $\textbf{V} \cup \textbf{W}$, where $\textbf{U}$ and $\textbf{W}$ are disjoint. The formulas for the
          new CPTs after arc reversal are as follows: $$\begin{aligned}
          {\textbf{P}}(Y{{\,|\,}}\textbf{U},\textbf{V},\textbf{W}) &amp;=&amp; \sum_x {\textbf{P}}(Y{{\,|\,}}\textbf{V},\textbf{W}, x) {\textbf{P}}(x{{\,|\,}}\textbf{U}, \textbf{V}) \\
          {\textbf{P}}(X{{\,|\,}}\textbf{U},\textbf{V},\textbf{W}, Y) &amp;=&amp; {\textbf{P}}(Y{{\,|\,}}X, \textbf{V}, \textbf{W}) {\textbf{P}}(X{{\,|\,}}\textbf{U}, \textbf{V}) / {\textbf{P}}(Y{{\,|\,}}\textbf{U},\textbf{V},\textbf{W})\ .\end{aligned}$$
          Prove that the new network expresses the same joint distribution
          over all variables as the original network.</p>
          </li>
          </ol>
          <p><strong>14.5</strong> Consider the Bayesian network in
          Figure <a href="#/">burglary-figure</a>.</p>
          <ol>
          <li><p>If no evidence is observed, are ${Burglary}$ and ${Earthquake}$
          independent? Prove this from the numerical semantics and from the
          topological semantics.</p>
          </li>
          <li><p>If we observe ${Alarm}{{\,{=}\,}}{true}$, are ${Burglary}$ and
          ${Earthquake}$ independent? Justify your answer by calculating
          whether the probabilities involved satisfy the definition of
          conditional independence.</p>
          </li>
          </ol>
          <p><strong>14.6</strong> Suppose that in a Bayesian network containing an unobserved variable
          $Y$, all the variables in the Markov blanket ${MB}(Y)$ have been
          observed.</p>
          <ol>
          <li><p>Prove that removing the node $Y$ from the network will not affect
          the posterior distribution for any other unobserved variable in
          the network.</p>
          </li>
          <li><p>Discuss whether we can remove $Y$ if we are planning to use (i)
          rejection sampling and (ii) likelihood weighting.</p>
          </li>
          </ol>
          <center>
          <b id="handedness-figure">Figure [handedness-figure]</b> Three possible structures for a Bayesian network describing genetic inheritance of handedness.
          </center><p><img src="https://cdn.rawgit.com/Nalinc/aima-exercises/notebooks/Jupyter%20notebook/figures/handedness1.svg" alt="handedness-figure"></p>
          <p><strong>14.7</strong> [handedness-exercise] Let $H_x$ be a random variable denoting the
          handedness of an individual $x$, with possible values $l$ or $r$. A
          common hypothesis is that left- or right-handedness is inherited by a
          simple mechanism; that is, perhaps there is a gene $G_x$, also with
          values $l$ or $r$, and perhaps actual handedness turns out mostly the
          same (with some probability $s$) as the gene an individual possesses.
          Furthermore, perhaps the gene itself is equally likely to be inherited
          from either of an individual’s parents, with a small nonzero probability
          $m$ of a random mutation flipping the handedness.</p>
          <ol>
          <li><p>Which of the three networks in
          Figure <a href="#handedness-figure">handedness-figure</a> claim that
          $ {\textbf{P}}(G_{{father}},G_{{mother}},G_{{child}}) = {\textbf{P}}(G_{{father}}){\textbf{P}}(G_{{mother}}){\textbf{P}}(G_{{child}})$?</p>
          </li>
          <li><p>Which of the three networks make independence claims that are
          consistent with the hypothesis about the inheritance of handedness?</p>
          </li>
          <li><p>Which of the three networks is the best description of the
          hypothesis?</p>
          </li>
          <li><p>Write down the CPT for the $G_{{child}}$ node in network (a), in
          terms of $s$ and $m$.</p>
          </li>
          <li><p>Suppose that
          $P(G_{{father}}{{\,{=}\,}}l)=P(G_{{mother}}{{\,{=}\,}}l)=q$. In
          network (a), derive an expression for $P(G_{{child}}{{\,{=}\,}}l)$
          in terms of $m$ and $q$ only, by conditioning on its parent nodes.</p>
          </li>
          <li><p>Under conditions of genetic equilibrium, we expect the distribution
          of genes to be the same across generations. Use this to calculate
          the value of $q$, and, given what you know about handedness in
          humans, explain why the hypothesis described at the beginning of
          this question must be wrong.</p>
          </li>
          </ol>
          <p><strong>14.8</strong> [markov-blanket-exercise] The <strong>Markov
          blanket</strong> of a variable is defined on page <a href="#/">markov-blanket-page</a>.
          Prove that a variable is independent of all other variables in the
          network, given its Markov blanket and derive
          Equation (<a href="#/">markov-blanket-equation</a>)
          (page <a href="#/">markov-blanket-equation</a>).</p>
          <center>
          <b id="car-starts-figure">Figure [car-starts-figure]</b> A Bayesian network describing some features of a car's electrical system and engine. Each variable is Boolean, and the *true* value indicates that the corresponding aspect of the vehicle is in working order.
          </center><p><img src="https://cdn.rawgit.com/Nalinc/aima-exercises/notebooks/Jupyter%20notebook/figures/car-starts.svg" alt="car-starts-figure"></p>
          <p><strong>14.9</strong> Consider the network for car diagnosis shown in
          Figure <a href="#car-starts-figure">car-starts-figure</a>.</p>
          <ol>
          <li><p>Extend the network with the Boolean variables ${IcyWeather}$ and
          ${StarterMotor}$.</p>
          </li>
          <li><p>Give reasonable conditional probability tables for all the nodes.</p>
          </li>
          <li><p>How many independent values are contained in the joint probability
          distribution for eight Boolean nodes, assuming that no conditional
          independence relations are known to hold among them?</p>
          </li>
          <li><p>How many independent probability values do your network tables
          contain?</p>
          </li>
          <li><p>The conditional distribution for ${Starts}$ could be described as
          a <strong>noisy-AND</strong> distribution. Define this
          family in general and relate it to the noisy-OR distribution.</p>
          </li>
          </ol>
          <p><strong>14.10</strong> Consider a simple Bayesian network with root variables ${Cold}$,
          ${Flu}$, and ${Malaria}$ and child variable ${Fever}$, with a
          noisy-OR conditional distribution for ${Fever}$ as described in
          Section <a href="#/">canonical-distribution-section</a>. By adding
          appropriate auxiliary variables for inhibition events and fever-inducing
          events, construct an equivalent Bayesian network whose CPTs (except for
          root variables) are deterministic. Define the CPTs and prove
          equivalence.</p>
          <p><strong>14.11</strong> [LG-exercise] Consider the family of linear Gaussian networks, as
          defined on page <a href="#/">LG-network-page</a>.</p>
          <ol>
          <li><p>In a two-variable network, let $X_1$ be the parent of $X_2$, let
          $X_1$ have a Gaussian prior, and let
          ${\textbf{P}}(X_2{{\,|\,}}X_1)$ be a linear
          Gaussian distribution. Show that the joint distribution $P(X_1,X_2)$
          is a multivariate Gaussian, and calculate its covariance matrix.</p>
          </li>
          <li><p>Prove by induction that the joint distribution for a general linear
          Gaussian network on $X_1,\ldots,X_n$ is also a
          multivariate Gaussian.</p>
          </li>
          </ol>
          <p><strong>14.12</strong> [multivalued-probit-exercise] The probit distribution defined on
          page <a href="#/">probit-page</a> describes the probability distribution for a Boolean
          child, given a single continuous parent.</p>
          <ol>
          <li><p>How might the definition be extended to cover multiple continuous
          parents?</p>
          </li>
          <li><p>How might it be extended to handle a <em>multivalued</em>
          child variable? Consider both cases where the child’s values are
          ordered (as in selecting a gear while driving, depending on speed,
          slope, desired acceleration, etc.) and cases where they are
          unordered (as in selecting bus, train, or car to get to work).
          (<em>Hint</em>: Consider ways to divide the possible values
          into two sets, to mimic a Boolean variable.)</p>
          </li>
          </ol>
          <p><strong>14.13</strong> In your local nuclear power station, there is an alarm that senses when
          a temperature gauge exceeds a given threshold. The gauge measures the
          temperature of the core. Consider the Boolean variables $A$ (alarm
          sounds), $F_A$ (alarm is faulty), and $F_G$ (gauge is faulty) and the
          multivalued nodes $G$ (gauge reading) and $T$ (actual core temperature).</p>
          <ol>
          <li><p>Draw a Bayesian network for this domain, given that the gauge is
          more likely to fail when the core temperature gets too high.</p>
          </li>
          <li><p>Is your network a polytree? Why or why not?</p>
          </li>
          <li><p>Suppose there are just two possible actual and measured
          temperatures, normal and high; the probability that the gauge gives
          the correct temperature is $x$ when it is working, but $y$ when it
          is faulty. Give the conditional probability table associated with
          $G$.</p>
          </li>
          <li><p>Suppose the alarm works correctly unless it is faulty, in which case
          it never sounds. Give the conditional probability table associated
          with $A$.</p>
          </li>
          <li><p>Suppose the alarm and gauge are working and the alarm sounds.
          Calculate an expression for the probability that the temperature of
          the core is too high, in terms of the various conditional
          probabilities in the network.</p>
          </li>
          </ol>
          <p><strong>14.14</strong> [telescope-exercise] Two astronomers in different parts of the world
          make measurements $M_1$ and $M_2$ of the number of stars $N$ in some
          small region of the sky, using their telescopes. Normally, there is a
          small possibility $e$ of error by up to one star in each direction. Each
          telescope can also (with a much smaller probability $f$) be badly out of
          focus (events $F_1$ and $F_2$), in which case the scientist will
          undercount by three or more stars (or if $N$ is less than 3, fail to
          detect any stars at all). Consider the three networks shown in
          Figure <a href="#telescope-nets-figure">telescope-nets-figure</a>.</p>
          <ol>
          <li><p>Which of these Bayesian networks are correct (but not
          necessarily efficient) representations of the preceding information?</p>
          </li>
          <li><p>Which is the best network? Explain.</p>
          </li>
          <li><p>Write out a conditional distribution for
          ${\textbf{P}}(M_1{{\,|\,}}N)$, for the case where
          $N{{\,{\in}\,}}\{1,2,3\}$ and $M_1{{\,{\in}\,}}\{0,1,2,3,4\}$. Each
          entry in the conditional distribution should be expressed as a
          function of the parameters $e$ and/or $f$.</p>
          </li>
          <li><p>Suppose $M_1{{\,{=}\,}}1$ and $M_2{{\,{=}\,}}3$. What are the
          <em>possible</em> numbers of stars if you assume no prior
          constraint on the values of $N$?</p>
          </li>
          <li><p>What is the <em>most likely</em> number of stars, given these
          observations? Explain how to compute this, or if it is not possible
          to compute, explain what additional information is needed and how it
          would affect the result.</p>
          </li>
          </ol>
          <p><strong>14.15</strong> Consider the network shown in
          Figure <a href="#telescope-nets-figure">telescope-nets-figure</a>(ii), and assume that the
          two telescopes work identically. $N{{\,{\in}\,}}\{1,2,3\}$ and
          $M_1,M_2{{\,{\in}\,}}\{0,1,2,3,4\}$, with the symbolic CPTs as described
          in Exercise <a href="#/">telescope-exercise</a>. Using the enumeration
          algorithm (Figure <a href="#/">enumeration-algorithm</a> on
          page <a href="#/">enumeration-algorithm</a>), calculate the probability distribution
          ${\textbf{P}}(N{{\,|\,}}M_1{{\,{=}\,}}2,M_2{{\,{=}\,}}2)$.</p>
          <center>
          <b id="telescope-nets-figure">Figure [telescope-nets-figure]</b> Three possible networks for the telescope problem.
          </center><p><img src="https://cdn.rawgit.com/Nalinc/aima-exercises/notebooks/Jupyter%20notebook/figures/telescope-nets.svg" alt="telescope-nets-figure"></p>
          <center>
          <b id="politics-figure">Figure [politics-figure]</b> A simple Bayes net with
          Boolean variables B = {BrokeElectionLaw}, I = {Indicted}, M = {PoliticallyMotivatedProsecutor}, G= {FoundGuilty}, J = {Jailed}.
          </center><p><img src="https://cdn.rawgit.com/Nalinc/aima-exercises/notebooks/Jupyter%20notebook/figures/politics.svg" alt="politics-figure"></p>
          <p><strong>14.16</strong> Consider the Bayes net shown in Figure <a href="#politics-figure">politics-figure</a>.</p>
          <ol>
          <li><p>Which of the following are asserted by the network
          <em>structure</em>?</p>
          <ol>
          <li><p>${\textbf{P}}(B,I,M) = {\textbf{P}}(B){\textbf{P}}(I){\textbf{P}}(M)$.</p>
          </li>
          <li><p>${\textbf{P}}(J{{\,|\,}}G) = {\textbf{P}}(J{{\,|\,}}G,I)$.</p>
          </li>
          <li><p>${\textbf{P}}(M{{\,|\,}}G,B,I) = {\textbf{P}}(M{{\,|\,}}G,B,I,J)$.</p>
          </li>
          </ol>
          </li>
          <li><p>Calculate the value of $P(b,i,\lnot m,g,j)$.</p>
          </li>
          <li><p>Calculate the probability that someone goes to jail given that they
          broke the law, have been indicted, and face a politically
          motivated prosecutor.</p>
          </li>
          <li><p>A <strong>context-specific independence</strong> (see
          page <a href="#/">CSI-page</a>) allows a variable to be independent of some of
          its parents given certain values of others. In addition to the usual
          conditional independences given by the graph structure, what
          context-specific independences exist in the Bayes net in
          Figure <a href="#politics-figure">politics-figure</a>?</p>
          </li>
          <li><p>Suppose we want to add the variable
          $P{{\,{=}\,}}{PresidentialPardon}$ to the network; draw the new
          network and briefly explain any links you add.</p>
          </li>
          </ol>
          <p><strong>14.17</strong> Consider the Bayes net shown in Figure <a href="#politics-figure">politics-figure</a>.</p>
          <ol>
          <li><p>Which, if any, of the following are asserted by the network
          <em>structure</em> (ignoring the CPTs for now)?</p>
          <ol>
          <li><p>${\textbf{P}}(B,I,M) = {\textbf{P}}(B){\textbf{P}}(I){\textbf{P}}(M)$.</p>
          </li>
          <li><p>${\textbf{P}}(J{{\,|\,}}G) = {\textbf{P}}(J{{\,|\,}}G,I)$.</p>
          </li>
          <li><p>${\textbf{P}}(M{{\,|\,}}G,B,I) = {\textbf{P}}(M{{\,|\,}}G,B,I,J)$.</p>
          </li>
          </ol>
          </li>
          <li><p>Calculate the value of $P(b,i,m,\lnot g,j)$.</p>
          </li>
          <li><p>Calculate the probability that someone goes to jail given that they
          broke the law, have been indicted, and face a politically
          motivated prosecutor.</p>
          </li>
          <li><p>A <strong>context-specific independence</strong> (see
          page <a href="#/">CSI-page</a>) allows a variable to be independent of some of
          its parents given certain values of others. In addition to the usual
          conditional independences given by the graph structure, what
          context-specific independences exist in the Bayes net in
          Figure <a href="#politics-figure">politics-figure</a>?</p>
          </li>
          <li><p>Suppose we want to add the variable
          $P{{\,{=}\,}}{PresidentialPardon}$ to the network; draw the new
          network and briefly explain any links you add.</p>
          </li>
          </ol>
          <p><strong>14.18</strong> [VE-exercise] Consider the variable elimination algorithm in
          Figure <a href="#/">elimination-ask-algorithm</a> (page <a href="#/">elimination-ask-algorithm</a>).</p>
          <ol>
          <li><p>Section <a href="#/">exact-inference-section</a> applies variable
          elimination to the query
          $${\textbf{P}}({Burglary}{{\,|\,}}{JohnCalls}{{\,{=}\,}}{true},{MaryCalls}{{\,{=}\,}}{true})\ .$$
          Perform the calculations indicated and check that the answer
          is correct.</p>
          </li>
          <li><p>Count the number of arithmetic operations performed, and compare it
          with the number performed by the enumeration algorithm.</p>
          </li>
          <li><p>Suppose a network has the form of a <em>chain</em>: a sequence
          of Boolean variables $X_1,\ldots, X_n$ where
          ${Parents}(X_i){{\,{=}\,}}\{X_{i-1}\}$ for $i{{\,{=}\,}}2,\ldots,n$.
          What is the complexity of computing
          ${\textbf{P}}(X_1{{\,|\,}}X_n{{\,{=}\,}}{true})$ using
          enumeration? Using variable elimination?</p>
          </li>
          <li><p>Prove that the complexity of running variable elimination on a
          polytree network is linear in the size of the tree for any variable
          ordering consistent with the network structure.</p>
          </li>
          </ol>
          <p><strong>14.19</strong> [bn-complexity-exercise] Investigate the complexity of exact inference
          in general Bayesian networks:</p>
          <ol>
          <li><p>Prove that any 3-SAT problem can be reduced to exact inference in a
          Bayesian network constructed to represent the particular problem and
          hence that exact inference is NP-hard. (<em>Hint</em>:
          Consider a network with one variable for each proposition symbol,
          one for each clause, and one for the conjunction of clauses.)</p>
          </li>
          <li><p>The problem of counting the number of satisfying assignments for a
          3-SAT problem is #P-complete. Show that exact inference is at least
          as hard as this.</p>
          </li>
          </ol>
          <p><strong>14.20</strong> [primitive-sampling-exercise] Consider the problem of generating a
          random sample from a specified distribution on a single variable. Assume
          you have a random number generator that returns a random number
          uniformly distributed between 0 and 1.</p>
          <ol>
          <li><p>Let $X$ be a discrete variable with
          $P(X{{\,{=}\,}}x_i){{\,{=}\,}}p_i$ for
          $i{{\,{\in}\,}}\{1,\ldots,k\}$. The <strong>cumulative distribution</strong> of $X$ gives the probability
          that $X{{\,{\in}\,}}\{x_1,\ldots,x_j\}$ for each possible $j$. (See
          also Appendix [math-appendix].) Explain how to
          calculate the cumulative distribution in $O(k)$ time and how to
          generate a single sample of $X$ from it. Can the latter be done in
          less than $O(k)$ time?</p>
          </li>
          <li><p>Now suppose we want to generate $N$ samples of $X$, where $N\gg k$.
          Explain how to do this with an expected run time per sample that is
          <em>constant</em> (i.e., independent of $k$).</p>
          </li>
          <li><p>Now consider a continuous-valued variable with a parameterized
          distribution (e.g., Gaussian). How can samples be generated from
          such a distribution?</p>
          </li>
          <li><p>Suppose you want to query a continuous-valued variable and you are
          using a sampling algorithm such as LIKELIHOODWEIGHTING to do the inference. How would
          you have to modify the query-answering process?</p>
          </li>
          </ol>
          <p><strong>14.21</strong> Consider the query
          ${\textbf{P}}({Rain}{{\,|\,}}{Sprinkler}{{\,{=}\,}}{true},{WetGrass}{{\,{=}\,}}{true})$
          in Figure <a href="#/">rain-clustering-figure</a>(a)
          (page <a href="#/">rain-clustering-figure</a>) and how Gibbs sampling can answer it.</p>
          <ol>
          <li><p>How many states does the Markov chain have?</p>
          </li>
          <li><p>Calculate the <strong>transition matrix</strong>
          ${\textbf{Q}}$ containing
          $q({\textbf{y}} \rightarrow {{\textbf{y}}'})$
          for all ${\textbf{y}}$, ${\textbf{y}}'$.</p>
          </li>
          <li><p>What does ${\textbf{ Q}}^2$, the square of the
          transition matrix, represent?</p>
          </li>
          <li><p>What about ${\textbf{Q}}^n$ as $n\to \infty$?</p>
          </li>
          <li><p>Explain how to do probabilistic inference in Bayesian networks,
          assuming that ${\textbf{Q}}^n$ is available. Is this a
          practical way to do inference?</p>
          </li>
          </ol>
          <p><strong>14.22</strong> [gibbs-proof-exercise] This exercise explores the stationary
          distribution for Gibbs sampling methods.</p>
          <ol>
          <li><p>The convex composition $[\alpha, q_1; 1-\alpha, q_2]$ of $q_1$ and
          $q_2$ is a transition probability distribution that first chooses
          one of $q_1$ and $q_2$ with probabilities $\alpha$ and $1-\alpha$,
          respectively, and then applies whichever is chosen. Prove that if
          $q_1$ and $q_2$ are in detailed balance with $\pi$, then their
          convex composition is also in detailed balance with $\pi$.
          (<em>Note</em>: this result justifies a variant of GIBBS-ASK in which
          variables are chosen at random rather than sampled in a
          fixed sequence.)</p>
          </li>
          <li><p>Prove that if each of $q_1$ and $q_2$ has $\pi$ as its stationary
          distribution, then the sequential composition
          $q {{\,{=}\,}}q_1 \circ q_2$ also has $\pi$ as its
          stationary distribution.</p>
          </li>
          </ol>
          <p><strong>14.23</strong> [MH-exercise] The <strong>Metropolis--Hastings</strong> algorithm is a member of the MCMC family; as such,
          it is designed to generate samples $\textbf{x}$ (eventually) according to target
          probabilities $\pi(\textbf{x})$. (Typically we are interested in sampling from
          $\pi(\textbf{x}){{\,{=}\,}}P(\textbf{x}{{\,|\,}}\textbf{e})$.) Like simulated annealing,
          Metropolis–Hastings operates in two stages. First, it samples a new
          state $\textbf{x'}$ from a <strong>proposal distribution</strong> $q(\textbf{x'}{{\,|\,}}\textbf{x})$, given the current state $\textbf{x}$.
          Then, it probabilistically accepts or rejects $\textbf{x'}$ according to the <strong>acceptance probability</strong>
          $$\alpha(\textbf{x'}{{\,|\,}}\textbf{x}) = \min\ \left(1,\frac{\pi(\textbf{x'})q(\textbf{x}{{\,|\,}}\textbf{x'})}{\pi(\textbf{x})q(\textbf{x'}{{\,|\,}}\textbf{x})}  \right)\ .$$
          If the proposal is rejected, the state remains at $\textbf{x}$.</p>
          <ol>
          <li><p>Consider an ordinary Gibbs sampling step for a specific variable
          $X_i$. Show that this step, considered as a proposal, is guaranteed
          to be accepted by Metropolis–Hastings. (Hence, Gibbs sampling is a
          special case of Metropolis–Hastings.)</p>
          </li>
          <li><p>Show that the two-step process above, viewed as a transition
          probability distribution, is in detailed balance with $\pi$.</p>
          </li>
          </ol>
          <p><strong>14.24</strong> [soccer-rpm-exercise]Three soccer teams $A$, $B$, and $C$, play each
          other once. Each match is between two teams, and can be won, drawn, or
          lost. Each team has a fixed, unknown degree of quality—an integer
          ranging from 0 to 3—and the outcome of a match depends probabilistically
          on the difference in quality between the two teams.</p>
          <ol>
          <li><p>Construct a relational probability model to describe this domain,
          and suggest numerical values for all the necessary
          probability distributions.</p>
          </li>
          <li><p>Construct the equivalent Bayesian network for the three matches.</p>
          </li>
          <li><p>Suppose that in the first two matches $A$ beats $B$ and draws with
          $C$. Using an exact inference algorithm of your choice, compute the
          posterior distribution for the outcome of the third match.</p>
          </li>
          <li><p>Suppose there are $n$ teams in the league and we have the results
          for all but the last match. How does the complexity of predicting
          the last game vary with $n$?</p>
          </li>
          <li><p>Investigate the application of MCMC to this problem. How quickly
          does it converge in practice and how well does it scale?</p>
          </li>
          </ol>

        </div>
</div>
</body>
</html>
